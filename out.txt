

--- PAGE 0 ---
ActiveLearningUsingPre-clustering
HieuT.Nguyen
TAT
@
SCIENCE
.
UVA
.
NL
ArnoldSmeulders
SMEULDERS
@
SCIENCE
.
UVA
.
NL
IntelligentSensoryInformationSystems,UniversityofAm
sterdam,FacultyofScience,Kruislaan403,NL-1098SJ,Am-
sterdam,TheNetherlands
Abstract
Thepaperisconcernedwithtwo-classactive

learning.Whilethecommonapproachforcol-

lectingdatainactivelearningistoselectsamples

closetotheclassicationboundary,betterper-

formancecanbeachievedbytakingintoaccount

thepriordatadistribution.Themaincontribution

ofthepaperisaformalframeworkthatincorpo-

ratesclusteringintoactivelearning.Thealgo-

rithmrstconstructsaclassieronthesetofthe

clusterrepresentatives,andthenpropagatesthe

classicationdecisiontotheothersamplesviaa

localnoisemodel.Theproposedmodelallows

toselectthemostrepresentativesamplesaswell

astoavoidrepeatedlylabelingsamplesinthe

samecluster.Duringtheactivelearningprocess,

theclusteringisadjustedusingthecoarse-to-ne

strategyinordertobalancebetweentheadvan-

tageoflargeclustersandtheaccuracyofthedata

representation.Theresultsofexperimentsinim-

agedatabasesshowabetterperformanceofour

algorithmcomparedtothecurrentmethods.
1.Introduction

Inrecentyears,researchinteresthasbeenattractedtosem
i-
supervisedlearningorlearningintheconditionthatonlya

smallinitialamountofdataislabeledwhilethemajority

ofthedataremainunlabeled.Whilemanymethodsfocus

toimprovethesupervisedlearningbyusingtheinforma-

tionfromunlabeleddata(Seeger,2001),anotherimportant

topicisagoodstrategyinselectingthedatatolabel,con-

sideringthatlabelingdataisatime-consumingjob.The

topicisknownas
activelearning
(Lewis&Gale,1994).
Considertheproblemoflearningabinaryclassierona

partiallylabeleddatabase


.Let
Appearingin
Proceedingsofthe

InternationalConference
onMachineLearning
,Banff,Canada,2004.Copyright2004by
theauthors.
bethelabeledsetinwhicheverysampleisgivenala-
bel


,and

.Theactivelearning
systemcomprisestwoparts:alearningengineandase-

lectionengine.Ateveryiterationthelearningengineuses

asupervisedlearningalgorithmtotrainaclassieron
.
Theselectionenginethenselectsasamplefrom

andre-
questsahumanexperttolabelthesamplebeforepassingit

tothelearningengine.Themajorgoalistoachieveagood

classierasbestaspossiblewithinareasonablenumberof

callsforlabelingbyhumanhelp.

Currentmethodsonactivelearningcanbecharacterizedby

theirbaselearningalgorithmswhichincludeprobabilisti
c
naiveBayes(Nigametal.,2000;Roy&McCallum,2001),

combinationofnaiveBayesandlogisticregression(Lewis

&Gale,1994),andtheSupportVectorMachine(SVM)

(Campbelletal.,2000;Tong&Koller,2001;Schohn&

Cohn,2000).ThenaiveBayesclassiersuffersfromtwo

problems.First,theclassierassumestheindependence

betweenthecomponentfeaturesof
.Thisassumptionis
oftenviolated.ThesecondproblemisthatnaiveBayesis

agenerativemodelforwhichtrainingreliesontheestima-

tionofthelikelihood
˘
ˇ
ˆ
.Thisestimationisinaccurate
inthecaseofactivelearningsincethetrainingdataarenot

randomlycollected.Thepaperfocusesondiscriminative

modelsincludinglogisticregressionandSVM.Thesemod-

elsaimtoestimatetheposteriorprobability
˘
ˇˆ.They
arelesssensitivetothewaythetrainingdataiscollected,

andhence,aremoresuitableforactivelearning.Amore

theoreticalconsiderationisgivenin(Zhang&Oles,2000).

Itiscrucialtochoosethemostﬁvaluableﬂtrainingsamples
.
Manymethodschoosethemostuncertainsampleswhich

areclosesttothecurrentclassicationboundary.Wename

thisapproachtheclosest-to-boundarycriterion.Thissim
-
pleandintuitivecriterionperformswellinsomeapplica-

tions(Lewis&Gale,1994;Tong&Chang,2001;Schohn

&Cohn,2000;Campbelletal.,2000).Someothercrite-

riahavebeenproposedspecicallyforSVM.In(Camp-

belletal.,2000),itisproposedtoselectthesamplethat

yieldsthelargestdecreaseofthemarginbetweenthetwo

classes.Themethodof(Tong&Koller,2001)selectsthe


--- PAGE 1 ---
samplethathalvesthepermittedregionoftheSVMparam-

etersintheparameterspace.Both(Campbelletal.,2000)

and(Tong&Koller,2001)needtopredictthevaluesof

theSVMparametersforeverypossiblecasewhereacan-

didatesamplemightbeaddedtothetrainingset.Sinceit

ishardtodothisefciently,thereferencesnallyresortt
o
theclosest-to-boundarycriterion.

Theclosest-to-boundarymethodsignorethepriordatadis-

tributionwhichcanbeusefulforactivelearning.In(Cohn

etal.,1996),itissuggestedtoselectsamplesthatminimiz
e
theexpectedfutureclassicationerror:
˘˘
ˆ˘
ˆˆˇ˘
ˆ(1)
where
˘
ˆisthetruelabelof
and
˘
ˆistheclassier
output.

ˇ
denotestheexpectationover
˘
ˇˆ.Dueto
thecomplexityoftheintegral,thedirectimplementationo
f
eq.(1)isusuallydifcult.However,itshowsthatthedata

uncertaintyshouldbeweightedwiththepriordensity
˘
ˆ.
If
˘
ˆisuniformorunknown,theexpectationunderthe
integralisthecontributionbyasampleintotheclassica-

tionerror.Theexpectationcanthenbeusedtomeasurethe

valueofthesampleintheconditionthatthecomputationof

theintegraliscomplex.Undertheassumptionthatthecur-

rentclassicationboundaryisgood,itiseasytoshowthat

theerrorexpectationismaximalforthesampleslyingon

theclassicationboundary,seesection3.3.When
˘
ˆis
knownandnon-uniform,theinformationaboutthedistri-

butioncanbeusedtoselectbetterdata.Inthispaper
˘
ˆisobtainedviaclusteringwhichcanbedoneofinewithout

theinteractionwithhuman.Theclusteringinformationis

thenusefulforactivelearningintwoways.First,therep-

resentativesampleslocatedincenterofclustersaremore

importantthantheother,andshouldbeselectedrstinla-

beling.Secondly,samplesinthesameclusterarelikelyto

havethesamelabel,(Seeger,2001;Chapelleetal.,2002).

Thisassumptionshouldbeusedtoaccelerateactivelearn-

ingbyreducingthenumberoflabelingsamplesfromthe

samecluster.

Theideatocombineclusteringandactivelearninghasap-

pearedinpreviouswork.In(McCallum&Nigam,1998),

anaiveBayesclassieristrainedoverbothlabeledand

unlabeleddatausinganEMalgorithm.Underthecon-

ditionthattheoverwhelmingmajorityofthedataisun-

labeled,thattrainingalgorithmamountstoclusteringthe

dataset,andtheroleofthelabeleddataisforinitializa-

tiononly.Clusteringinformationalsocontributestothe

selectionwhereanuncertaintymeasureisweightedwith

thedensityofthesample.Thereferencedapproachdoes

notmatch,however,theobjectiveofthispapertocombine

clusteringwithadiscriminativemodel.Severalotheracti
ve
learningschemesalsoweightheuncertaintywiththedata

density(Zhang&Chen,2002;Tangetal.,2002).Some
methodsputmoreemphasisonthesamplerepresentative-

nessbyselectingclustercentersfromasetofmostinterest
-
ingsamples.Intherepresentativesamplingby(Xuetal.,

2003),thealgorithmusesthek-meansalgorithmtoclus-

terthesampleslyingwithinthemarginofaSVMclassier

trainedonthecurrentlabeledset.Thesamplesatcluster

centersarethenselectedforhumanlabeling.Themethod

of(Shen&Zhai,2003)hasasimilaridea,butappliesthek-

medoidalgorithmforthetoprelevantsamples.Ingeneral,

heuristicmethodshavebeenproposedtobalancebetween

theuncertaintyandtherepresentativenessoftheselected

sample.Theyencouragetheselectionofclustercenters.

However,nomeasurehasbeentakentoavoidrepeatedly

labelingsamplesinsamecluster.Inaddition,thereareim-

portantquestionsthatremainopen,namely,howtoadapt

theclassicationmodelforatrainingsetthatcontainsonl
y
clustercenters?and,howtoclassifysamplesthataredis-

putedbyseveralclusters?Thispaperpresentsasolution

fortheseissuesusingamathematicalmodelthatexplicitly

takesclusteringintoaccount.

Theorganizationofthepaperisasfollows.Section2de-

scribestheincorporationoftheclusteringinformationin
to
thedatamodel,andprovidesthetheoreticalframeworkfor

thedataclassication.Section3presentsouractivelearn
-
ingalgorithm.Section4showstheresultsofthealgorithm

fortheclassicationofimagesintestdatabases.

2.Probabilisticframework

2.1.Datamodel

Inthestandardclassication,datagenerationisdescribe
d
bythejointdistribution
˘

ˆ
ofthedata
andtheclass
label
.Theclusteringinformationisex-
plicitlyincorporatedbyintroducingthehiddenclusterla
-
bel


,where
isthenumberofclustersin
thedata.
indicatesthatthesamplebelongstothe
-th
cluster.Assumethatallinformationabouttheclasslabel
isalreadyencodedintheclusterlabel
.Thisimplies
thatonce
isknown,
and
areindependent.Thejoint
distributioniswrittenas:
˘

ˆ˘
ˇ
ˆ˘
ˇ
ˆ˘
ˆ˘
ˇˆ˘
ˇ
ˆ˘
ˆ(2)
ThesimpleBayesianbeliefnetrepresentingthemodelis
kcluster labelyclass labelxdataFigure1.
TheBayesiannetforthedatamodel.
depictedinFigure1.


--- PAGE 2 ---
Beforegivingthespecicformforthethreedistributions

ineq.(2)weremarkthatasimilarschemehasbeenpro-

posedforthepassivesemi-supervisedlearning(Miller&

Uyar,1996;Seeger,2001).Theconceptualdifference,

however,betweentheirapproachandoursisinthede-

nitionof
˘
ˇˆ.Inthereferences,
˘
ˇˆisdenedwithin
individualclusters.Asaconsequence,theestimationofth
e
parametersof
˘
ˇˆcanbeunreliableduetoinsufcient
labeleddatainacluster.Inourmodel,
˘
ˇˆisdenedfor
allclusterswiththesameparameters.

Weuselogisticregressionfor
˘
ˇˆ:
˘
ˇˆ
˘
ˆ(3)
Here,
isarepresentativeofthe
-thclusterwhichisde-
terminedviaclustering.

and
arethelogistic
regressionparameters.Inessence,
˘
ˇˆisthelabelmodel
forarepresentativesubsetofthedatabase.

Intheidealcasewheredataiswellclustered,onceallthe

parametersof
˘
ˇˆaredetermined,onecouldusethis
probabilitytodeterminethelabeloftheclusterrepresent
a-
tives,andthenassignthesamelabeltotheremainingsam-

plesinthecluster.Inpractice,however,clusteringcanbe

inaccurateandwewillhaveproblemswithclassicationof

samplesatborderbetweentheclusters.Toachievebetter

classicationforthosesamples,weuseasoftclustermem-

bershipwhichallowsasampletobeconnectedtomorethan

oneclusters(representatives)withaprobability.Thenoi
se
distribution
˘
ˇ
ˆisthenusedtopropagateinformationof
label
fromtherepresentativesintotheremainingmajor-
ityofthedata,seeFigure2.WeusetheisotropicGaussian

model:
˘
ˇ
ˆ˘ˆ
(4)
where
isthevarianceassumedtobethesameforall
clusters.
representativesrepresentativesclassifier for theactual classifier-++Figure2.
Theclassicationmodel
Let
˘
ˆ.Then,
˘
ˆisamixtureof
Gaussians
withtheweights
.
Inthepresentedmethod,theparameters
,
,
and
are
estimatedfromthedata.Thescaleparameter
isgiven
initially.Itcanbechangedduringactivelearningwhena

differentclusteringsettingisneeded.

2.2.Dataclassication

Giventheabovemodel,onecalculates
˘
ˇˆ,theposterior
probabilityoflabelofasampleasfollows:
˘
ˇˆ˘
ˇˆ˘
ˇˆ˘
ˇˆ(5)
where
˘
ˇˆ˘
ˇ
ˆ˘
ˆ˘
ˆ.
DataarethenclassiedusingtheBayesdecisionrule:
˘
ˆif
˘
ˇˆ˘
ˇˆotherwise
(6)
where
denotethecurrentestimatesoftheparameters.
Observefromeq.(5)thattheclassicationdecisionisa

weightedcombinationoftheclassicationdecisionforthe

representatives.Wellclusteredsampleswillbeassigned

thesamelabelasthenearestrepresentative.Samplesdis-

putedbyseveralclusters,ontheotherhand,willbeas-

signedthelabelofthecluster,whichhasthehighestcon-

dence.Notethattheweights
˘
ˇˆarexedunlessthe
dataarere-clusteredwhereas
˘
ˇˆisupdateduponthe
arrivalofnewtrainingdata.

3.Descriptionofalgorithm

Theparametersofthemodelproposedinsection2.1are

estimatedvialikelihoodmaximization.Thedatalikelihoo
d
comprisestwoparts:thelikelihoodofthelabeleddataand

thelikelihoodoftheunlabeleddata:
˘
ˇˆ
˙˝˘
ˆ˘
ˇ˛
˙˝˘
ˆ(7)
where
˚and
˚denotethesetofindicesoflabeledand
unlabeledsamplesrespectively.Expanding
˙˝˘
ˆas
thesumof
˙˝˘
ˆand
˙˝˘
ˇˆ,thelikelihood(7)can
bewrittenwithexplicitdependenceontheparametersas

follows:
˘

ˆ˘
ˇˆ˜
ˇ˛˙˝
˘



ˆ˘
ˇˆ
˙˝˘
ˇ

ˆ(8)
Astheamountoftheunlabeleddataisoverwhelming

overthelabeleddata,theparameters

and


--- PAGE 3 ---

aredeterminedmainlybymaximizingtherst
termineq.(8).Themaximizationofeachtermcanthere-

forebedoneseparately.Theclusteringalgorithmmaxi-

mizesthelikelihoodofthedatasamplestoobtaintheclus-

terrepresentativesandtheclusterprior.Themaximizatio
n
ofthelabellikelihoodfollowstoestimatetheparameters
and
.
(section 3.2)Initial clustering
(section 3.1)unlabeled sample, eq. (30)Selecting and labeling anCalculating p(y|x)
eq. (5)Cluster adjustmentSTOP?YESENDNOEstimating p(y|k)data samples(section 3.4)Figure3.
Theproposedactivelearningalgorithm.
TheblockschemeofthealgorithmisillustratedinFigure3.

3.1.Initialclustering

Inthepresentedalgorithm,thegoalofclusteringisdata

representationratherthandataclassication.Wetherefo
re
usethe
-medoidalgorithmof(Kaufman&Rousseeuw,
1990).Thealgorithmnds
representatives

ofthedataset

soastominimizethesumofthe
distancefromthedatasamplestothenearestrepresenta-

tive.See(Struyfetal.,1997)forthedetailedimplementa-

tion.

The
-medoidalgorithmiscomputationallyexpensive
wheneither
or
islarge.Inpracticalapplicationsboth
numbersareverylargeindeed.Thefollowingsimpli-

cationsareemployedtoreducecomputations.First,the

datasetissplitintosmallersubsets.The
-medoidalgo-
rithmisthenappliedtoclustereverysubsetintoalimited

number
ofclusters.Clusteringiscontinuedbysubse-
quentlybreakingtheclusterwiththelargestradius
into
twosmalleroneswith:

˘
ˇˇˇ(9)
where
˚denotesthesetofindicesofthesamplesinthe
-thcluster.Theprocessofclusterssioniscompleted
when:

(10)
where
isapredenedconstant.Wehaveused
.
Thus,theclustersizeandthenalnumberofclusters
is
controlledbythescaleparameter
.
Oncetheclusterrepresentatives

havebeende-
termined,theclusterprior
isobtainedbyiteratingthe
followingtwoequationsuntilstability:
˘
ˇˆ
˘
ˇˆˆ
˘
ˆ(11)
˘
ˇˆ(12)
3.2.Theestimationoftheclasslabelmodel

Thissectionpresentstheestimationofthedistribution
˘
ˇˆbasedonthemaximizationofthesecondlikelihood
ineq.(8).Fixingtheclusterrepresentatives
,thelikeli-
hooddependsonlyontheparameters
and
:

˙˝˘
ˇˆ
˙˝˘
ˇ
ˆ(13)
Fromeq.(5),
˘
ˇ
ˆcanbewrittenasamixtureof
logisticdistributions
˘
ˇ
ˆwiththeweights
˘
ˇˆ.
Incasethedimensionality
ishigherthanthenumberof
thelabeledsamples,theoptimizationin(13)isnumericall
y
unstable.Theconventionalapproachtoovercometheprob-

lemistoaddaregularizationterm
˛where
˛isa
predenedparameter.Thisleadstotheminimizationofthe

followingobjectivefunction:
˚˘ˆ˛˘
ˇˆ
˙˝˜˘
ˇˆ˘
ˇ
ˆ (14)
Remarkthateq.(14)istheextensionoftheregularized

logisticregression(Zhang&Oles,2001;Zhu&Hastie,

2001)forthemixtureofthelogisticdistributions.

Theminimizationof
˚isimplementedusingNewton'sal-
gorithmwhichguaranteestondalocalminimum.Fur-

thermore,since
˚isconvex,ithasonlyonelocalminimum
whichisalsotheglobalminimum.

Startingwithaninitialguess
and
,theparameters
and
areupdatediteratively.Ateachiteration,theparam-


--- PAGE 4 ---
eterincrementinthesteepestdirectionis:
˚(15)
where
˚istheJacobianof
˚,and
isapositivedenite
approximationofthehessianmatrixof
˚.Usingeq.(3),it
canbeshownthat:
˚

˝˛(16)
where
˘
ˇˆ
˘
ˇ
ˆ˘
ˇ
ˆ˘
ˇˆ˘
ˇ
ˆ(17)
Forthehessianmatrix,thefollowingapproximationcanbe

used:
˛(18)
where
istheidentitymatrixand:
˘
ˇˆ
˘
ˇ
ˆ˘
ˇ
ˆ˘
ˇˆ˘
ˇ
ˆ(19)
Togetmoreinsightintoeq.(15),let:
(20)

(21)

˛(22)
Here,
isthe
matrixwhosecolumnsarethevectors
.Onecanshowthat:
˚˚(23)
˚˚ˇ˚(24)
If
ishigh,itisefcienttoinvert
usingtheWoodbury
formula:
˛˘˛ˆ(25)
Ifall
arenon-zero,thesizeof
is
.Since
islarge,theinvertingof
asineq.(25)wouldbecomputa-
tionallyexpensivestill.However,remarkthatforasample
thereareonlyfewvaluesof
suchthat
˘
ˇˆisdiffer-
entfromzero,especiallyif
isaclusterrepresentative.In
thelattercase,thesampletypicallybelongstoonecluster
only.Aswillbeseeninthenextsubsection,thepresented

algorithmtendstoselectthetrainingdatafromthecluster

representatives.Thenumberofnon-zero
isthensmall,
approximatelythesameasthenumberoflabeledsamples.

Thecomputationof
ineq.(25)canthenbedoneef-
cientlybysuppressingthecolumnsin
whichcorrespond
to
thatequaltozero.
3.3.Criterionfordataselection

Theselectioncriteriongivesprioritytotwotypesofsam-

ples:samplesclosetotheclassicationboundaryandsam-

pleswhichareclusterrepresentatives.Furthermore,with
in
thesetofclusterrepresentatives,oneshouldstartwithth
e
highestdensityclustersrst.

Wehavenotedthatthecomputationofthefutureclassica-

tionerrorineq.(1)iscomplicated.So,insteadofchoosing

thesamplethatproducesthesmallestfutureerror,wese-

lectthesamplethathasthelargestcontributiontothecur-

renterror.Althoughsuchapproachdoesnotguaranteethe

smallestfutureerror,thereisagoodchanceforalargede-

creaseoftheerror.Theselectioncriterionis:

˘
ˇ˛
˘ˆˇ˘
ˆ(26)
where
denotestheindexoftheselectedsample.
Theerrorexpectationforanunlabeled
iscalculatedover
thedistribution
˘
ˇˆ:
˘ˆˇ˘
ˇˆ˘
ˆ˘
ˇˆ˘
ˆ˘˘
ˇˆ˘
ˇˆˆ(27)
Itshouldbenotedthattheprobability
˘
ˇˆisun-
knownandneedstobeapproximated.Anobviouschoiceis

tousethecurrentestimation
˘
ˇˆ,assuming
aregoodenough.Letting
˘ˆ˘
ˇˆ˘
ˇˆ(28)
itfollowsfromeq.(5)that:
˘ˆˇ˘ˇ˘ˆˇˆ(29)
Observethatif
liesonthecurrentclassicationbound-
ary,thequantity
ˇ˘ˆˇisminimal,andhencetheexpected
errorismaximal.

Eq.(26)becomes:

˘
ˇ˛
˘ˇ˘ˆˇˆ˘
ˆ(30)
where
˘
ˆ
(31)


--- PAGE 5 ---
Figure4.
Exampleviewofimagesintherstdatabase.
Theresultingcriterionindeedsatisesthedemandsputin

thebeginningofthesubsection.Theterm
˘ˇ˘ˆˇˆgivesprioritytothesamplesattheboundary.Meantime,
˘
ˆgivesprioritytotherepresentativesofdenseclusters.
3.4.Coarse-to-neadjustmentofclustering

Thelabelingofhighdensityclusterspromisesasubstantia
l
moveoftheclassicationboundary.Itisthereforeadvanta
-
geoustogroupthedataintolargeclustersintheinitialclu
s-
tering.Thisisachievedbysettingahighvaluefortheini-

tialscaleparameter
.Whentheclassicationboundary
reachestheborderbetweentheglobalclusters,anerclus-

teringwithsmallerclustersizeisbettertoobtainamore

accurateclassicationboundary.Themaximumineq.(30)

canbeusedastheindicationfortheneedtoadjusttheclus-

tering.Ifthisquantitydropsbelowathreshold

:

˘
ˇ˛
˘ˇ˘ˆˇˆ˘
ˆ
(32)
thescaleparameterisdecreased:
(33)
where
.Thedatasetisthenre-clustered.The
parameters

and
arepredened.Wehaveused

and
.Notethatclusteringthedatasetwithdifferent
scalescanbedoneofine.Furthermore,changeofthescale

takesplacenotineveryiteration,butonlyfewtimesduring

thelearningprocess.

4.Experiments

Wehaveperformedtwoexperimentstotesttheperfor-

manceoftheproposedalgorithm.Intherstexperiment,

thealgorithmisappliedtondhumanfaceimagesina

databasecontaining2500imagesofsize
.See
(Phametal.,2002)fordetailsonhowtheimageswerecre-

ated.ExampleviewsofsomeimagesareshowninFigure4.

Inthesecondexperiment,atestdatabasewasmadeofim-

agesofhandwrittendigitstakenfromtheMNISTdatabase
(http://yann.lecun.com/exdb/mnist/).Thesizeofimages
is
.Theobjectiveistoseparatetheimagesofagiven
digitagainsttheothernine.

Intheexperimentsthefollowingsettingwasused.Theim-

agesareconsideredasthevectorscomposedofthepixel

greyvalueswhichrangefrom0to255.Theinitialtrain-

ingsetcontainsequalnumbersofobjectandnon-object

images.Theinitialsizeofthissetwas

andwasin-
creasedto
duringactivelearning,where
isthe
numberofsamplesinthedatabase.Forclustering,the

databasesweresplitintosubsetsper1250samples.The
-medoidalgorithmwasappliedforeachsubsetwiththe
initialnumberofclusters
.Theinitialvalueof
was
where
isthenumberofpixelsinone
image.Fortheestimationoftheclasslabeldistribution,w
e
usetheregularizationcoefcient
˛.
Everytimeanewtrainingsampleisadded,theclassier

isre-trainedandtestedontherestofthedatabase.The

classicationerroriscalculatedasthesumofthemissed

positivesandfalsealarmsrelativeto
.Theperformance
evaluationisbasedonthedecreaseoftheclassicationer-

rorasthefunctionoftheamountoftrainingsamples.

Forcomparison,wehavealsoimplementedthreeother

activelearningalgorithms.Theyusethestandardlinear

SVMforclassication.Therstalgorithmselectstraining

datarandomly.Inthesecondalgorithm,dataareselected

accordingtotheclosest-to-boundarycriterion.Thethird

algorithmusestherepresentativesamplingof(Xuetal.,

2003)whichselectsthemedoidcentersintheSVMmargin.

Asweselectonesampleperiteration,thisleadstoselec-

tionofthemostrepresentativesampleamongthesamples

inthemargin.

Figure5showstheresultoftherstexperimentfordiffer-

entproportionsbetweenthenumbersoffaceandnon-face

imagesinthedatabase.Figure6showstheresultofthe

secondexperimentforthedifferentsizesofthedatabase.

Bothguresshowtheaverageoftheclassicationerror

obtainedbyrepeatingtheexperimentswiththreedifferent

initialtrainingsetsthatarepickeduprandomly.Theresul
ts
ofFigure6arealsoanaverageoverthetendigits.

Theproposedalgorithmoutperformsallthreeotheralgo-

rithms.Themostsignicantimprovementisobservedin

Figure5awithequalnumbersforobjectandnon-object

samplesinthedatabase.Theimprovementdecreaseswhen

theamountoftheobjectsamplesissmallrelativetothe

non-objectsamples,seeFigure5c.Inthiscase,sincethere

arenoclustersoftheobjectclass,theproposedalgorithm

isnotadvantageousovertheclosest-to-boundaryalgorith
m
inndingobjectsamples.Nevertheless,theproposedalgo-

rithmremainsbetterasitstillbenetsfromtheclustering

ofnon-objectsamples.Representativesamplingturnsout


--- PAGE 6 ---
a)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn1=1250, n2=1250random sampling
closest-to-boundary
representative sampling
proposed algorithmb)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn1=500, n2=2000random sampling
closest-to-boundary
representative sampling
proposed algorithmc)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn1=125, n2=2375random sampling
closest-to-boundary
representative sampling
proposed algorithmFigure5.
Theresultsfortheclassicationoffaceimages.

and

arethenumberoffaceandnon-faceimagesinthedatabases
respectively.

a)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn=1250random sampling
closest-to-boundary
representative sampling
proposed algorithmb)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn=2500random sampling
closest-to-boundary
representative sampling
proposed algorithmc)
0.511.522.533.500.020.040.060.080.10.120.140.160.180.2percentage of labeled data (%)averaged classification errorn=5000random sampling
closest-to-boundary
representative sampling
proposed algorithmFigure6.
TheclassicationresultsofhandwrittendigitsfromtheMN
ISTdatabase.
isthedatabasesize.
toperformbetteronlythanrandomsampling.Apossible

reasoncouldbetheundervaluationoftheuncertaintyand

thelackofaproperclassicationmodel.

5.Conclusion

Thepaperhasproposedaformalmodelforincorporation

ofclusteringintoactivelearning.Themodelallowsto

selectmostrepresentativetrainingexamplesaswellasto

avoidrepeatedlylabelingsamplesinsamecluster,leading

tobetterperformancethanthecurrentmethods.Totake

theadvantageofthesimilaritybetweentheclasslabelof

datainthesamecluster,themethodrstconstructsaclas-

sieroverthepopulationoftheclusterrepresentatives.W
e
useregularizedlogisticregressionwhichisadiscrimina-

tivemodelwithstate-of-the-artperformanceandwhichis

naturallyttedintoaprobabilisticframework.Thegaus-

siannoisemodelisthenusedtoinfertheclasslabelfor

non-representativesamples.Newtrainingdataareselecte
d
fromthesampleshavingthemaximalcontributiontothe

currentexpectederror.Inadditiontoclosenesstotheclas
-
sicationboundary,theselectioncriteriongivespriorit
y
alsototherepresentativesofthedenseclusters,makingth
e
trainingsetstatisticallystable.

Themethodwasrestrictedtolinearlogisticregressionas
themainpurposeofthepaperistoshowtheadvantageof

usingclusteringinformation.Wehavesucceededinthat

goalsforthegivendatasets.

References

Campbell,C.,Cristianini,N.,&Smola,A.(2000).Query
learningwithlargemarginclassiers.
Proc.17thIn-
ternationalConf.onMachineLearning
(pp.111Œ118).
MorganKaufmann,CA.
Chapelle,O.,Weston,J.,&Scholkopf,B.(2002).Cluster
kernelsforsemi-supervisedlearning.
AdvancesinNeu-
ralInformationProcessingSystems
.
Cohn,D.A.,Ghahramani,Z.,&Jordan,M.I.(1996).Ac-
tivelearningwithstatisticalmodels.
JournalofArticial
Intelligenceresearch
,
4
,129Œ145.
Kaufman,L.,&Rousseeuw,P.(1990).
Findinggroupsin
data:Anintroductiontoclusteranalysis
.JohnWiley&
Sons.
Lewis,D.D.,&Gale,W.A.(1994).Asequentialalgo-
rithmfortrainingtextclassiers.
ProceedingsofSIGIR-
94,17thACMInternationalConferenceonResearch


--- PAGE 7 ---
andDevelopmentinInformationRetrieval
(pp.3Œ12).
SpringerVerlag.
McCallum,A.K.,&Nigam,K.(1998).EmployingEMin
pool-basedactivelearningfortextclassication.
Proc.
15thInternationalConf.onMachineLearning
(pp.350Œ
358).MorganKaufmann,CA.
Miller,D.,&Uyar,H.(1996).Amixtureofexpertsclassi-
erwithlearningbasedonbothlabelledandunlabelled

data.
AdvancesinNeuralInformationProcessingSys-
tems9
(pp.571Œ577).
Nigam,K.,McCallum,A.,Thrun,S.,&Mitchell,T.
(2000).Textclassicationfromlabeledandunlabeled

documentsusingEM.
MachineLearning
,
39
,103Œ134.
Pham,T.,Worring,M.,&Smeulders,A.(2002).Facede-
tectionbyaggregatedbayesiannetworkclassiers.
Pat-
ternRecogn.Letters
,
23
,451Œ461.
Roy,N.,&McCallum,A.(2001).Towardoptimalactive
learningthroughsamplingestimationoferrorreduction.

Proc.18thInternationalConf.onMachineLearning
(pp.
441Œ448).MorganKaufmann,CA.
Schohn,G.,&Cohn,D.(2000).Lessismore:Active
learningwithsupportvectormachines.
Proc.17thIn-
ternationalConf.onMachineLearning
(pp.839Œ846).
MorganKaufmann,CA.
Seeger,M.(2001).
Learningwithlabeledandunlabeled
data
(TechnicalReport).EdinburghUniversity.
Shen,X.,&Zhai,C.(2003).Activefeedback-UIUC
TREC-2003HARDexperiments.
The12thTextRe-
trievalConference,TREC
.
Struyf,A.,Hubert,M.,&Rousseeuw,P.(1997).Inte-
gratingrobustclusteringtechniquesins-plus.
Compu-
tationalStatisticsandDataAnalysis
,
26
,17Œ37.
Tang,M.,Luo,X.,&Roukos,S.(2002).Activelearning
forstatisticalnaturallanguageparsing.
Proc.oftheAsso-
ciationforComputationalLinguistics40thAnniversary

Meeting
.Philadelphia,PA.
Tong,S.,&Chang,E.(2001).Supportvectormachineac-
tivelearningforimageretrieval.
Proceedingsofthe9th
ACMint.conf.onMultimedia
(pp.107Œ118).Ottawa.
Tong,S.,&Koller,D.(2001).Supportvectormachine
activelearningwithapplicationstotextclassication.

JournalofMachineLearningResearch
,
2
,45Œ66.
Xu,Z.,Yu,K.,Tresp,V.,Xu,X.,&Wang,J.(2003).Rep-
resentativesamplingfortextclassicationusingsupport

vectormachines.
25thEuropeanConf.onInformation
RetrievalResearch,ECIR2003
.Springer.
Zhang,C.,&Chen,T.(2002).Anactivelearningframe-
workforcontent-basedinformationretrieval.
IEEEtrans
onmultimedia
,
4
,260Œ268.
Zhang,T.,&Oles,F.(2000).Aprobabilityanalysisonthe
valueofunlabeleddataforclassicationproblems.
Proc.
Int.Conf.onMachineLearning
.
Zhang,T.,&Oles,F.J.(2001).Textcategorizationbased
onregularizedlinearclassicationmethods.
Information
Retrieval
,
4
,5Œ31.
Zhu,J.,&Hastie,T.(2001).Kernellogisticregressionand
theimportvectormachine.
AdvancesinNeuralInforma-
tionProcessingSystems
.
